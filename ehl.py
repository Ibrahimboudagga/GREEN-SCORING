# -*- coding: utf-8 -*-
"""Copie de automl.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iPJgTLyNegk41W2pS57R3fZ8U16jMK8h

TPOT is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming.
"""

!pip install tpot

import tpot
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import sklearn

import seaborn as sns
#importing libraries
import pandas as pd 
import numpy as np
#regular expresions
import re
# Suppress warnings from pandas
import warnings
warnings.filterwarnings('ignore')

# utilities
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold

from sklearn import preprocessing
# modeling 

import datetime
from sklearn.model_selection import GridSearchCV

now = datetime.datetime.now()
import matplotlib.pyplot as plt

data = pd.read_excel(r'EHL.xlsx')

y = data['Expert_Score']
x= data.drop(['id', 'h_name','Expert_Score'],axis=1)

"""we have to encode all the categorical feature """

from  sklearn.preprocessing import OrdinalEncoder as oe
encoding = oe()
x[['b_comment']] = encoding.fit_transform(x[['b_comment']])

for col in x.columns :
    x[col]= x[col].apply(lambda a: str(a).replace(',', '.')) 
    x[col]= x[col].apply(lambda a: float(a))

x = x.astype('float')

"""To avoid being misled, all correlated features must be disabled."""

l = ['b_rating','b_r_staff', 'b_r__confort','b_r_equipment','tripad_rate','fb_n_followers','b_r_quality_price']
x = x.drop(l, axis = 1)

plt.figure(figsize = (20,100))
plt.title('heatmap')
sns.heatmap(x.corr(),linewidths=1,vmax=1.0, 
            square=True,  linecolor='white', annot=True)

x['Expert_Score'] = y

"""Below is another type of encoding.
- x['Expert_Score']< 4 --> x['Expert_Score'] =0
- 4 =< x['Expert_Score'] < 8 --> x['Expert_Score'] =1
- 8 =< x['Expert_Score']  --> x['Expert_Score'] =2 

"""

idx =[]
for i in x.index:
 if x['Expert_Score'][i] != 'na':
    idx.append(i)
    if float(x['Expert_Score'][i])<4 : 
        x['Expert_Score'][i] =0
    if float(x['Expert_Score'][i])>=4 and float(x['Expert_Score'][i])<8: 
        x['Expert_Score'][i] =1
    if float(x['Expert_Score'][i])>=8 : 
        x['Expert_Score'][i] =2

x1 = x[x['Expert_Score']!='na']

"""The idea is to split the hotels that have an 'Expert_Score' != 'na' into three groups based on their 'Expert_Score':

- group1 = ('Expert_Score' = 0).
- group2 = ('Expert_Score' = 1).
- group3 = ('Expert_Score' = 2).
"""

def find_centers(ds1):
    
    ds1 = np.matrix(ds1)
    
    center = np.sum(ds1, axis = 0)/ds1.shape[0]
    return center

"""We have to determine the center of each cluster."""

x1_0 = x1[x1['Expert_Score']==0]
x1_0 = x1_0.drop('Expert_Score', axis = 1)
center0 = find_centers(x1_0)
x1_1 = x1[x1['Expert_Score']==1]
x1_1 = x1_1.drop('Expert_Score', axis = 1)
center1 = find_centers(x1_1)
x1_2= x1[x1['Expert_Score']==2]
x1_2 = x1_2.drop('Expert_Score', axis = 1)
center2 = find_centers(x1_2)

"""- -First we have to measure the distance between each hotel and each cluster's center. 
- Then we have to assign each hotel to the cluster with which it has the minimum distance.
"""

for i in x.index :
 li =[] 
 if x['Expert_Score'][i] == 'na':
    for col in x.columns:
        if col != 'Expert_Score':
            li.append(x[col][i])
    li = np.array(li)
    d0 = np.linalg.norm(li-center0, -np.inf)
    d1 = np.linalg.norm(li-center1, -np.inf)
    d2 = np.linalg.norm(li-center2, -np.inf)
    if min(d0,d1,d2) == d0:
        x['Expert_Score'][i]=0
    if min(d0,d1,d2) == d1:
        x['Expert_Score'][i]=1
    if min(d0,d1,d2) == d2:
        x['Expert_Score'][i]=2

x['Expert_Score'].value_counts()

"""We have to repeat this operation many times to equilibrate the data."""

def up_v(ds,col):
    x_0 = ds[ds[col]==0]
    x_0 = x_0.drop(col, axis = 1)
    center0 = find_centers(x_0)
    x_1 = ds[ds[col]==1]
    x_1 = x_1.drop(col, axis = 1)
    center1 = find_centers(x_1)
    x_2= ds[ds['Expert_Score']==2]
    x_2 = x_2.drop(col, axis = 1)
    center2 = find_centers(x_2)
    
    for i in ds.index :
     if i not in idx : 
         li =[] 
         if ds[col][i] ==1 :
            for cl in ds.columns:
                if cl != col:
                    li.append(x[cl][i])
            li = np.array(li)
            d0 = np.linalg.norm(li-center0, -np.inf)
            d1 = np.linalg.norm(li-center1, -np.inf)
            d2 = np.linalg.norm(li-center2, -np.inf)
            if min(d0,d1,d2) == d0:
                ds[col][i]=0
            if min(d0,d1,d2) == d1:
                ds[col][i]=1
            if min(d0,d1,d2) == d2:
                ds[col][i]=2
    return ds

for i in range(10):
  x = up_v(x,'Expert_Score')
  print(x['Expert_Score'].value_counts())

def find_centers1(ds1,a):
    
    ds1 = np.matrix(ds1)
    
    center = np.sum(ds1[:a,:], axis = 0)/a
    return center

def up_v1(ds,col,a):
    x_0 = ds[ds[col]==0]
    x_0 = x_0.drop(col, axis = 1)
    center0 = find_centers1(x_0,a)
    x_1 = ds[ds[col]==1]
    x_1 = x_1.drop(col, axis = 1)
    center1 = find_centers1(x_1,a)
    x_2= ds[ds[col]==2]
    x_2 = x_2.drop(col, axis = 1)
    center2 = find_centers1(x_2,a)
    
    for i in ds.index :
     if i not in idx : 
         li =[] 
         if ds[col][i] ==1 :
            for cl in ds.columns:
                if cl != col:
                    li.append(x[cl][i])
            li = np.array(li)
            d0 = np.linalg.norm(li-center0, -np.inf)
            d1 = np.linalg.norm(li-center1, -np.inf)
            d2 = np.linalg.norm(li-center2, -np.inf)
            if min(d0,d1,d2) == d0:
                ds[col][i]=0
            if min(d0,d1,d2) == d1:
                ds[col][i]=1
            if min(d0,d1,d2) == d2:
                ds[col][i]=2
    return ds

for i in range(10):  
  for i in range(10): 
    k = x['Expert_Score'].value_counts()[0]
    j = x['Expert_Score'].value_counts()[2]

    #x = up_v1(x,'Expert_Score',k)
    x = up_v1(x,'Expert_Score',j)
    #x['Expert_Score'].value_counts()
  for i in range(10): 
    k = x['Expert_Score'].value_counts()[0]
    j = x['Expert_Score'].value_counts()[2]

    x = up_v1(x,'Expert_Score',k)
    #x = up_v1(x,'Expert_Score',j)
    #x['Expert_Score'].value_counts()

x['Expert_Score'].value_counts()



from tpot import TPOTRegressor,TPOTClassifier

tpot_automl = TPOTClassifier(generations=100, population_size=100, 
                            verbosity=2, scoring='r2',   
                            max_time_mins=8, random_state = 666)

x = x.astype('float')

df = x

df.columns

y1 = df['Expert_Score']
x1 = df.drop(columns=['Expert_Score'], axis = 1)
x1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, test_size =0.25, shuffle = True)

#tpot_automl.fit(x1_train, y1_train)

#tpot_automl.export('tpot_best_model.py')

import numpy as np
import pandas as pd
from sklearn.cluster import FeatureAgglomeration
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from tpot.export_utils import set_param_recursive

# NOTE: Make sure that the outcome column is labeled 'target' in the data file
tpot_data = df
features = tpot_data.drop('Expert_Score', axis=1)
training_features, testing_features, training_target, testing_target = \
            train_test_split(features, tpot_data['Expert_Score'], random_state=666)

# Average CV score on the training set was: 0.23259323238001306
exported_pipeline = make_pipeline(
    FeatureAgglomeration(affinity="euclidean", linkage="complete"),
    RandomForestClassifier(bootstrap=False, criterion="gini", max_features=0.6500000000000001, min_samples_leaf=2, min_samples_split=19, n_estimators=100)
)
# Fix random state for all the steps in exported pipeline
set_param_recursive(exported_pipeline.steps, 'random_state', 666)

exported_pipeline.fit(training_features, training_target)
results = exported_pipeline.predict(testing_features)
results1 = exported_pipeline.predict(training_features)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
print('metrics for test_data')
print('********************')
print(confusion_matrix(testing_target,results))
print(classification_report(testing_target,results))
print(accuracy_score(testing_target,results))

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
print('metrics for train_data')
print('********************')
print(confusion_matrix(training_target,results1))
print(classification_report(training_target,results1))
print(accuracy_score(training_target,results1))

